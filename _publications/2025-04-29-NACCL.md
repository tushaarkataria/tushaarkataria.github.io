---
title: "Ntsebench: Cognitive reasoning benchmark for vision language models"
collection: publications
permalink: /publication/2025-04-29-NACCL
excerpt: 'Cognitive textual and visual reasoning tasks, including puzzles, series, and analogies, demand the ability to quickly reason, decipher, and evaluate patterns both textually and spatially. Due to extensive training on vast amounts of human-curated data, large language models (LLMs) and vision language models (VLMs) excel in common-sense reasoning tasks, but still struggle with more complex reasoning that demands deeper cognitive understanding. We introduce NTSEBENCH, a new dataset designed to evaluate cognitive multimodal reasoning and problem-solving skills of large models. The dataset contains 2,728 multiple-choice questions, accompanied by a total of 4,642 images, spanning 26 categories. These questions are drawn from the nationwide NTSE examination in India and feature a mix of visual and textual general aptitude challenges, designed to assess intelligence and critical thinking skills beyond mere rote learning. We establish baselines on the dataset using state-of-the-art LLMs and VLMs. To facilitate a comparison between open-source and propriety models, we propose four distinct modeling strategies to handle different modalities—text and images—in the dataset instances.'
 
date: 2025-04-29
venue: 'Nations of the Americas Chapter of the Association for Computational Linguistics'
paperurl: 'https://aclanthology.org/2025.findings-naacl.204/'
archive: https://arxiv.org/abs/2407.10380
citation: 'Pandya, Pranshu, Vatsal Gupta, Agney S. Talwarr, Tushar Kataria, Dan Roth, and Vivek Gupta. "Ntsebench: Cognitive reasoning benchmark for vision language models." In Findings of the Association for Computational Linguistics: NAACL 2025, pp. 3680-3708. 2025.' 
---
